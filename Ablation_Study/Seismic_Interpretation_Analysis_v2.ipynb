{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a3e1a1",
   "metadata": {},
   "source": [
    "# Seismic Interpretation Analysis v2\n",
    "\n",
    "This notebook implements a modular approach to seismic interpretation using various deep learning models. It supports an ablation study to compare different model architectures and combinations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The workflow consists of the following steps:\n",
    "1. Load preprocessed seismic data\n",
    "2. Configure models and training parameters\n",
    "3. Run ablation study with different model combinations\n",
    "4. Analyze and visualize results\n",
    "\n",
    "This modular approach allows for easy experimentation with different model architectures and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1a2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838136ea-b7cd-467c-8d35-d9ff3ada2f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fuller_m\n",
      "/home/fuller_m/ablationstudy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/home/fuller_m/ablationstudy')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a3e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import model_utils\n",
    "from utils import extract_traces_from_patches, extract_random_trace_from_patch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1a4",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Instead of preprocessing the data in this notebook, we load preprocessed data that was created using the `preprocess_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bab6cc-a112-4db1-b992-5fabf22f54b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport numpy as np\\nimport warnings\\n\\n# Suppress specific NumPy warnings if needed\\nwarnings.filterwarnings(\\'ignore\\', category=RuntimeWarning, message=\\'.*invalid value.*\\')\\nwarnings.filterwarnings(\\'ignore\\', category=RuntimeWarning, message=\\'.*divide by zero.*\\')\\n\\nfrom preprocess_data import load_and_preprocess_data\\n\\n# Set up parameters\\nbase_data_dir = \"F3_Demo_2020\"\\nsegy_filename = \"Rawdata/Seismic_data.sgy\"\\nhorizon_subdir = \"Rawdata/Surface_data\"\\nhorizon_filenames = [\\n    \"F3-Horizon-FS4.xyt.bz2\",\\n    \"F3-Horizon-MFS4.xyt\",\\n    \"F3-Horizon-FS6.xyt\",\\n    \"F3-Horizon-FS7.xyt\",\\n    \"F3-Horizon-FS8.xyt\",\\n    \"F3-Horizon-Shallow.xyt\",\\n    \"F3-Horizon-Top-Foresets.xyt\"\\n]\\npatch_size = 32\\nstride = 16\\nmax_patches = 50000  # Consider reducing this if memory issues occur\\n\\n# Construct full paths\\nsegy_path = os.path.join(base_data_dir, segy_filename)\\nhorizon_dir = os.path.join(base_data_dir, horizon_subdir)\\nhorizon_paths = [os.path.join(horizon_dir, hf) for hf in horizon_filenames]\\n\\ntry:\\n    # Call the function with the correct parameters\\n    # Wrap in try/except to catch and report errors\\n    X_patches, y_labels, num_classes = load_and_preprocess_data(\\n        segy_path, \\n        horizon_paths, \\n        patch_size, \\n        stride, \\n        max_patches\\n    )\\n    \\n    # Now you can use X_patches, y_labels, and num_classes directly in your notebook\\n    print(f\"X_patches shape: {X_patches.shape}\")\\n    print(f\"y_labels shape: {y_labels.shape}\")\\n    print(f\"Number of classes: {num_classes}\")\\n    \\n    # Save the results if needed\\n    output_dir = \"preprocessed_data\"\\n    output_filename = \"preprocessed_seismic_data.npz\"\\n    os.makedirs(output_dir, exist_ok=True)\\n    output_path = os.path.join(output_dir, output_filename)\\n    np.savez_compressed(\\n        output_path,\\n        X_patches=X_patches,\\n        y_labels=y_labels,\\n        num_classes=num_classes\\n    )\\n    print(\"Preprocessing and saving completed successfully!\")\\n    \\nexcept Exception as e:\\n    print(f\"Error during preprocessing: {type(e).__name__}: {e}\")\\n    \\n    # If it\\'s a memory error, suggest reducing max_patches\\n    if isinstance(e, MemoryError):\\n        print(\"Memory error encountered. Try reducing max_patches or processing in smaller batches.\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only used once to preprocess dataset\n",
    "'''import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress specific NumPy warnings if needed\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*invalid value.*')\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, message='.*divide by zero.*')\n",
    "\n",
    "from preprocess_data import load_and_preprocess_data\n",
    "\n",
    "# Set up parameters\n",
    "base_data_dir = \"F3_Demo_2020\"\n",
    "segy_filename = \"Rawdata/Seismic_data.sgy\"\n",
    "horizon_subdir = \"Rawdata/Surface_data\"\n",
    "horizon_filenames = [\n",
    "    \"F3-Horizon-FS4.xyt.bz2\",\n",
    "    \"F3-Horizon-MFS4.xyt\",\n",
    "    \"F3-Horizon-FS6.xyt\",\n",
    "    \"F3-Horizon-FS7.xyt\",\n",
    "    \"F3-Horizon-FS8.xyt\",\n",
    "    \"F3-Horizon-Shallow.xyt\",\n",
    "    \"F3-Horizon-Top-Foresets.xyt\"\n",
    "]\n",
    "patch_size = 32\n",
    "stride = 16\n",
    "max_patches = 50000  # Consider reducing this if memory issues occur\n",
    "\n",
    "# Construct full paths\n",
    "segy_path = os.path.join(base_data_dir, segy_filename)\n",
    "horizon_dir = os.path.join(base_data_dir, horizon_subdir)\n",
    "horizon_paths = [os.path.join(horizon_dir, hf) for hf in horizon_filenames]\n",
    "\n",
    "try:\n",
    "    # Call the function with the correct parameters\n",
    "    # Wrap in try/except to catch and report errors\n",
    "    X_patches, y_labels, num_classes = load_and_preprocess_data(\n",
    "        segy_path, \n",
    "        horizon_paths, \n",
    "        patch_size, \n",
    "        stride, \n",
    "        max_patches\n",
    "    )\n",
    "    \n",
    "    # Now you can use X_patches, y_labels, and num_classes directly in your notebook\n",
    "    print(f\"X_patches shape: {X_patches.shape}\")\n",
    "    print(f\"y_labels shape: {y_labels.shape}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "    # Save the results if needed\n",
    "    output_dir = \"preprocessed_data\"\n",
    "    output_filename = \"preprocessed_seismic_data.npz\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        X_patches=X_patches,\n",
    "        y_labels=y_labels,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    print(\"Preprocessing and saving completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during preprocessing: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    # If it's a memory error, suggest reducing max_patches\n",
    "    if isinstance(e, MemoryError):\n",
    "        print(\"Memory error encountered. Try reducing max_patches or processing in smaller batches.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a3e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from preprocessed_data/preprocessed_seismic_data.npz...\n",
      "Loaded 50000 samples with 8 classes.\n",
      "X_patches shape: (50000, 1, 32, 32, 32)\n",
      "y_labels shape: (50000,)\n",
      "X_traces shape: (50000, 32)\n"
     ]
    }
   ],
   "source": [
    "def load_preprocessed_data(data_path):\n",
    "    \"\"\"Load preprocessed seismic data from a .npz file.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the preprocessed data file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_patches, y_labels, num_classes, class_names)\n",
    "    \"\"\"\n",
    "    print(f\"Loading preprocessed data from {data_path}...\")\n",
    "    \n",
    "    try:\n",
    "        data = np.load(data_path)\n",
    "        X_patches = data['X_patches']\n",
    "        y_labels = data['y_labels']\n",
    "        num_classes = int(data['num_classes'])\n",
    "        \n",
    "        # Load class names if available\n",
    "        class_names = None\n",
    "        if 'class_names' in data:\n",
    "            class_names = data['class_names']\n",
    "        \n",
    "        print(f\"Loaded {X_patches.shape[0]} samples with {num_classes} classes.\")\n",
    "        print(f\"X_patches shape: {X_patches.shape}\")\n",
    "        print(f\"y_labels shape: {y_labels.shape}\")\n",
    "        \n",
    "        return X_patches, y_labels, num_classes, class_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        # Create dummy data for demonstration if real data is not available\n",
    "        print(\"Creating dummy data for demonstration...\")\n",
    "        num_samples = 100\n",
    "        patch_depth = 32\n",
    "        patch_height = 32\n",
    "        patch_width = 32\n",
    "        num_classes = 8\n",
    "        \n",
    "        X_patches = np.random.rand(num_samples, 1, patch_depth, patch_height, patch_width).astype(np.float32)\n",
    "        y_labels = np.random.randint(0, num_classes, size=num_samples).astype(np.int64)\n",
    "        class_names = [f\"Class_{i}\" for i in range(num_classes)]\n",
    "        \n",
    "        print(f\"Created {num_samples} dummy samples with {num_classes} classes.\")\n",
    "        print(f\"X_patches shape: {X_patches.shape}\")\n",
    "        print(f\"y_labels shape: {y_labels.shape}\")\n",
    "        \n",
    "        return X_patches, y_labels, num_classes, class_names\n",
    "\n",
    "# Load preprocessed data\n",
    "data_path = config.DATA_CONFIG['preprocessed_data_path']\n",
    "X_patches, y_labels, num_classes, class_names = load_preprocessed_data(data_path)\n",
    "\n",
    "# Extract traces for sequence models\n",
    "X_traces = extract_traces_from_patches(X_patches, num_traces_per_patch=5, seed=42)\n",
    "print(f\"X_traces shape: {X_traces.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1a6",
   "metadata": {},
   "source": [
    "## 3. Create DataLoaders\n",
    "\n",
    "Create DataLoaders for training, validation, and testing using the utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a3e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 35000 samples\n",
      "Validation loader: 7500 samples\n",
      "Test loader: 7500 samples\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "train_loader, val_loader, test_loader = model_utils.create_dataloaders(\n",
    "    X_patches, y_labels, X_traces,\n",
    "    batch_size=config.TRAINING_PARAMS['batch_size'],\n",
    "    train_split=config.DATA_CONFIG['train_split'],\n",
    "    val_split=config.DATA_CONFIG['val_split'],\n",
    "    test_split=config.DATA_CONFIG['test_split'],\n",
    "    random_seed=config.DATA_CONFIG['random_seed'],\n",
    "    stratify=config.DATA_CONFIG['stratify']\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader.dataset)} samples\")\n",
    "print(f\"Validation loader: {len(val_loader.dataset)} samples\")\n",
    "print(f\"Test loader: {len(test_loader.dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1a8",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Configuration\n",
    "\n",
    "The configuration module (`config.py`) contains definitions for all available models and their parameters. Here we can review and modify the configuration if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a3e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CNN models:\n",
      "  - cnn3d: Original 3D CNN with two convolutional layers\n",
      "  - resnet3d: 3D ResNet with residual connections for deeper feature extraction\n",
      "  - attention_unet3d: 3D U-Net with attention gates for focused feature extraction\n",
      "  - patchnet3d: Multi-scale 3D CNN with dilated convolutions for capturing features at different scales\n",
      "\n",
      "Available sequence models:\n",
      "  - bilstm: Bidirectional LSTM for processing seismic traces\n",
      "  - lstm: Unidirectional LSTM for processing seismic traces\n",
      "  - transformer: Transformer model with self-attention for processing seismic traces\n",
      "  - wide_deep_transformer: Transformer with parallel wide (shallow) and deep paths for capturing different patterns\n",
      "  - custom_transformer: Custom transformer implementation with flexible attention mechanisms\n",
      "\n",
      "Hybrid model:\n",
      "  - HybridModel: Hybrid model combining CNN and sequence models\n",
      "\n",
      "Ablation configuration:\n",
      "  - CNN models: ['cnn3d', 'resnet3d', 'attention_unet3d', 'patchnet3d']\n",
      "  - Sequence models: ['bilstm', 'lstm', 'transformer', 'wide_deep_transformer', 'custom_transformer']\n",
      "  - Run all combinations: True\n"
     ]
    }
   ],
   "source": [
    "# Display available CNN models\n",
    "print(\"Available CNN models:\")\n",
    "for key, model_config in config.CNN_MODELS.items():\n",
    "    print(f\"  - {key}: {model_config['description']}\")\n",
    "\n",
    "print(\"\\nAvailable sequence models:\")\n",
    "for key, model_config in config.SEQ_MODELS.items():\n",
    "    print(f\"  - {key}: {model_config['description']}\")\n",
    "\n",
    "print(\"\\nHybrid model:\")\n",
    "print(f\"  - {config.HYBRID_MODEL['class_name']}: {config.HYBRID_MODEL['description']}\")\n",
    "\n",
    "# Display ablation configuration\n",
    "print(\"\\nAblation configuration:\")\n",
    "print(f\"  - CNN models: {config.ABLATION_CONFIG['cnn_models']}\")\n",
    "print(f\"  - Sequence models: {config.ABLATION_CONFIG['seq_models']}\")\n",
    "print(f\"  - Run all combinations: {config.ABLATION_CONFIG['run_all_combinations']}\")\n",
    "if not config.ABLATION_CONFIG['run_all_combinations']:\n",
    "    print(f\"  - Specific combinations: {config.ABLATION_CONFIG['specific_combinations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1aa",
   "metadata": {},
   "source": [
    "## 5. Single Model Testing (Optional)\n",
    "\n",
    "Before running the full ablation study, we can test individual models to ensure they work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a3e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single CNN model\n",
    "def test_cnn_model(cnn_key):\n",
    "    print(f\"Testing CNN model: {cnn_key}\")\n",
    "    cnn_config = config.CNN_MODELS[cnn_key]\n",
    "    model = model_utils.load_model_from_config(cnn_config, num_classes=num_classes)\n",
    "    print(model)\n",
    "    \n",
    "    # Test with a batch from the dataloader\n",
    "    batch = next(iter(train_loader))\n",
    "    if len(batch) == 3:  # X_patches, X_traces, y\n",
    "        X_patches, _, y = batch\n",
    "    else:  # X_patches, y\n",
    "        X_patches, y = batch\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_patches)\n",
    "    \n",
    "    print(f\"Input shape: {X_patches.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Expected output shape: [batch_size, {num_classes}]\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test a single sequence model\n",
    "def test_seq_model(seq_key):\n",
    "    print(f\"Testing sequence model: {seq_key}\")\n",
    "    seq_config = config.SEQ_MODELS[seq_key]\n",
    "    model = model_utils.load_model_from_config(seq_config, num_classes=num_classes)\n",
    "    print(model)\n",
    "    \n",
    "    # Test with a batch from the dataloader\n",
    "    batch = next(iter(train_loader))\n",
    "    if len(batch) == 3:  # X_patches, X_traces, y\n",
    "        _, X_traces, y = batch\n",
    "    else:  # X_patches, y\n",
    "        X_patches, y = batch\n",
    "        # Extract traces for testing\n",
    "        X_traces = torch.tensor(np.array([extract_random_trace_from_patch(patch.numpy()) \n",
    "                                         for patch in X_patches]), dtype=torch.float32)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_traces)\n",
    "    \n",
    "    print(f\"Input shape: {X_traces.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Expected output shape: [batch_size, {num_classes}]\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test a hybrid model\n",
    "def test_hybrid_model(cnn_key, seq_key):\n",
    "    print(f\"Testing hybrid model: {cnn_key} + {seq_key}\")\n",
    "    cnn_config = config.CNN_MODELS[cnn_key]\n",
    "    seq_config = config.SEQ_MODELS[seq_key]\n",
    "    \n",
    "    # Load individual models\n",
    "    cnn_model = model_utils.load_model_from_config(cnn_config, num_classes=num_classes)\n",
    "    seq_model = model_utils.load_model_from_config(seq_config, num_classes=num_classes)\n",
    "    \n",
    "    # Load hybrid model\n",
    "    hybrid_model = model_utils.load_hybrid_model(\n",
    "        cnn_config, seq_config, config.HYBRID_MODEL, num_classes=num_classes\n",
    "    )\n",
    "    print(hybrid_model)\n",
    "    \n",
    "    # Test with a batch from the dataloader\n",
    "    batch = next(iter(train_loader))\n",
    "    if len(batch) == 3:  # X_patches, X_traces, y\n",
    "        X_patches, X_traces, y = batch\n",
    "    else:  # X_patches, y\n",
    "        X_patches, y = batch\n",
    "        # Extract traces for testing\n",
    "        X_traces = torch.tensor(np.array([extract_random_trace_from_patch(patch.numpy()) \n",
    "                                         for patch in X_patches]), dtype=torch.float32)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = hybrid_model(X_patches, X_traces)\n",
    "    \n",
    "    print(f\"CNN input shape: {X_patches.shape}\")\n",
    "    print(f\"Sequence input shape: {X_traces.shape}\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Expected output shape: [batch_size, {num_classes}]\")\n",
    "    \n",
    "    return hybrid_model\n",
    "\n",
    "# Uncomment to test individual models\n",
    "# cnn_model = test_cnn_model('cnn3d')\n",
    "# seq_model = test_seq_model('bilstm')\n",
    "# hybrid_model = test_hybrid_model('cnn3d', 'bilstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1ac",
   "metadata": {},
   "source": [
    "## 6. Run Ablation Study\n",
    "\n",
    "Run the ablation study with all specified model combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training cnn3d_bilstm ===\n",
      "\n",
      "Initializing HybridModel...\n",
      "  CNN model: SeismicCNN3D\n",
      "  Sequence model: SeismicBiLSTM\n",
      "  CNN feature size: 16384\n",
      "  Sequence feature size: 128\n",
      "  Combined feature size: 16512\n",
      "  Fusion hidden size: 128\n",
      "  Number of classes: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ba752a62c24b12b3c7f6f56c05b191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5e6db5763141849e9ed172f7153890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss: 1.2669, Train Acc: 0.5369, Val Loss: 0.8280, Val Acc: 0.6744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe743ee5c5a344f183616b646ebcad64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df7573296df48a48ea39a10226de65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: Train Loss: 0.9590, Train Acc: 0.6146, Val Loss: 0.7069, Val Acc: 0.7241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd010d769ba34fdf8d720396fd39225c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91d393daeb140fbb316059fc59133f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: Train Loss: 0.8814, Train Acc: 0.6408, Val Loss: 0.6774, Val Acc: 0.7251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7692f0e5262c40d6820cd8414c80e747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed511169b3140c798548b5bb435176a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: Train Loss: 0.8260, Train Acc: 0.6549, Val Loss: 0.7125, Val Acc: 0.6909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b62c40b3c7458ea93b5dec3313af09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52824e1482a5424691dbeb2905fefe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: Train Loss: 0.8016, Train Acc: 0.6623, Val Loss: 0.6319, Val Acc: 0.7321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7983d421564d2dbf1eaa702270d1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6f28da115043188ab460c76b9e6f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: Train Loss: 0.7610, Train Acc: 0.6763, Val Loss: 0.5484, Val Acc: 0.7529\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e240b411331c4dbab2ef7753c21e7061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a751c43ab13448eb86c4ec6992d965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: Train Loss: 0.7523, Train Acc: 0.6801, Val Loss: 0.5841, Val Acc: 0.7339\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798ef7ffd7c947729fe372428433a270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28c0b72c83c48c68c864f31bce0ef01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: Train Loss: 0.7419, Train Acc: 0.6863, Val Loss: 0.5353, Val Acc: 0.7703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6ff587a8204cf78c5bf3537389d9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24bbed0dc5746f8ae2b71a6caa522de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: Train Loss: 0.7142, Train Acc: 0.6987, Val Loss: 0.5542, Val Acc: 0.7613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b708f8d7ffa4474c91d0ca4f561477e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fca5e6a265d45ccaf7992aa9387d101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: Train Loss: 0.6845, Train Acc: 0.7096, Val Loss: 0.4986, Val Acc: 0.7755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d02e455e054e9bbb0410d138a26140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead94be158ff44b388de067a714ccbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: Train Loss: 0.6680, Train Acc: 0.7164, Val Loss: 0.4743, Val Acc: 0.7871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ac6c37ce9e45b09f96821728ede2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf86bfcc8b6434e938b766056fb7f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: Train Loss: 0.6371, Train Acc: 0.7292, Val Loss: 0.4659, Val Acc: 0.7944\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a366966ea1dd48c98db4f8fd1d10b9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7338c466ce6742908d967ec229fba00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: Train Loss: 0.6275, Train Acc: 0.7312, Val Loss: 0.4670, Val Acc: 0.7937\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f8849461b5414f8d8b5f095c450d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a103833525e4ea0a6142d9fa41c2c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: Train Loss: 0.5900, Train Acc: 0.7478, Val Loss: 0.4278, Val Acc: 0.8144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6f9607d99249439cc34dab17b8f738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eb1919635f457aaae9364ef6f90b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: Train Loss: 0.5709, Train Acc: 0.7570, Val Loss: 0.5054, Val Acc: 0.7907\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41825fcab86a4f0cbb4f1e573ad4bf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b71008146654d1ba7bbe0b801de1b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: Train Loss: 0.5394, Train Acc: 0.7707, Val Loss: 0.4677, Val Acc: 0.8033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24489023c8454061b03d931844a76bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e86123b976d4d6d8bc65ff427c2df06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: Train Loss: 0.4957, Train Acc: 0.7911, Val Loss: 0.4234, Val Acc: 0.8253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75205951d2214bc1a3fff6cc27171f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db50ea60fe834432b59781079e4db6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: Train Loss: 0.4699, Train Acc: 0.8005, Val Loss: 0.6307, Val Acc: 0.7963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b992140f595f45198899450655278fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60cac1f6e4449c0bc3672f92625b2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: Train Loss: 0.4425, Train Acc: 0.8141, Val Loss: 0.3866, Val Acc: 0.8349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a425d3eba14b447fbd408e3ce51a2978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e66176a9ca47df8b52e652c4d6b671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: Train Loss: 0.4259, Train Acc: 0.8196, Val Loss: 0.3682, Val Acc: 0.8469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b0b4cca0d743cb9a7ac5213ac3c70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb4fe2811624b7f87e75d8742d5e010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: Train Loss: 0.4048, Train Acc: 0.8296, Val Loss: 0.3892, Val Acc: 0.8328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79937650f97e4b9599fc3a7aca1e2b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0afbfd8c9e4c8883ecc72d3be81ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: Train Loss: 0.3849, Train Acc: 0.8366, Val Loss: 0.4382, Val Acc: 0.8243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230b0f39893c4cd780efaeac16253529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b074eeee3e8d44078e4b7f631bb917db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: Train Loss: 0.3640, Train Acc: 0.8461, Val Loss: 0.3860, Val Acc: 0.8428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a939d4d18f4d64872a58892d60825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dafa721f8a4ede8d0e119142f28367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: Train Loss: 0.3412, Train Acc: 0.8556, Val Loss: 0.3784, Val Acc: 0.8505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c82cdfcb874e7080d8deda75f6855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7ee1f2c01943bf9537150bf8269c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: Train Loss: 0.3145, Train Acc: 0.8671, Val Loss: 0.3858, Val Acc: 0.8496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8df7ff1968437a941bf417138e6b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad62182237c4580ab0631251deb52aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/50 [Val]:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: Train Loss: 0.2966, Train Acc: 0.8741, Val Loss: 0.4840, Val Acc: 0.8323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f03a681b624b88b011e983bd23fe36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/50 [Train]:   0%|          | 0/1094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run ablation study\n",
    "results_df = model_utils.run_ablation_study(\n",
    "    config, X_patches, y_labels, X_traces,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    results_dir=config.ABLATION_CONFIG['results_dir']\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAblation Study Results:\")\n",
    "display(results_df[['model_name', 'accuracy', 'precision', 'recall', 'f1', 'cohen_kappa', 'balanced_accuracy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1ae",
   "metadata": {},
   "source": [
    "## 7. Analyze Results\n",
    "\n",
    "Analyze the results of the ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy comparison\n",
    "plt.figure(figsize=config.VISUALIZATION_CONFIG['figsize'])\n",
    "sns.barplot(x='accuracy', y='model_name', data=results_df.sort_values('accuracy', ascending=False))\n",
    "plt.title('Model Comparison - Accuracy')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 score comparison\n",
    "plt.figure(figsize=config.VISUALIZATION_CONFIG['figsize'])\n",
    "sns.barplot(x='f1', y='model_name', data=results_df.sort_values('f1', ascending=False))\n",
    "plt.title('Model Comparison - F1 Score')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1b0",
   "metadata": {},
   "source": [
    "## 8. Detailed Analysis of Best Model\n",
    "\n",
    "Analyze the best performing model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on accuracy\n",
    "best_model_row = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "best_model_name = best_model_row['model_name']\n",
    "best_cnn = best_model_row['cnn_model']\n",
    "best_seq = best_model_row['seq_model']\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"CNN model: {best_cnn} - {config.CNN_MODELS[best_cnn]['description']}\")\n",
    "print(f\"Sequence model: {best_seq} - {config.SEQ_MODELS[best_seq]['description']}\")\n",
    "print(f\"Accuracy: {best_model_row['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {best_model_row['f1']:.4f}\")\n",
    "\n",
    "# Load confusion matrix for the best model\n",
    "results_dir = config.ABLATION_CONFIG['results_dir']\n",
    "cm_path = os.path.join(results_dir, f\"{best_model_name}_confusion_matrix.{config.VISUALIZATION_CONFIG['save_format']}\")\n",
    "\n",
    "if os.path.exists(cm_path):\n",
    "    plt.figure(figsize=config.VISUALIZATION_CONFIG['figsize'])\n",
    "    img = plt.imread(cm_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Confusion matrix image not found at {cm_path}\")\n",
    "    \n",
    "    # Load the best model and generate confusion matrix\n",
    "    cnn_config = config.CNN_MODELS[best_cnn]\n",
    "    seq_config = config.SEQ_MODELS[best_seq]\n",
    "    \n",
    "    # Load best model checkpoint if available\n",
    "    best_model_path = os.path.join(results_dir, f\"{best_model_name}_best.pth\")\n",
    "    if os.path.exists(best_model_path):\n",
    "        # Load hybrid model\n",
    "        best_model = model_utils.load_hybrid_model(\n",
    "            cnn_config, seq_config, config.HYBRID_MODEL, num_classes=num_classes\n",
    "        )\n",
    "        best_model.load_state_dict(torch.load(best_model_path))\n",
    "        best_model = best_model.to(device)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        results = model_utils.evaluate_model(\n",
    "            best_model, test_loader, config.EVALUATION_METRICS,\n",
    "            device=device, class_names=class_names\n",
    "        )\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        model_utils.plot_confusion_matrix(\n",
    "            results['confusion_matrix'],\n",
    "            class_names=class_names,\n",
    "            figsize=config.VISUALIZATION_CONFIG['figsize'],\n",
    "            cmap=config.VISUALIZATION_CONFIG['cmap'],\n",
    "            normalize=True\n",
    "        )\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Best model checkpoint not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1b2",
   "metadata": {},
   "source": [
    "## 9. Training Curves Analysis\n",
    "\n",
    "Analyze the training curves of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot training history for the best model\n",
    "history_path = os.path.join(results_dir, f\"{best_model_name}_history.json\")\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    model_utils.plot_training_history(history, figsize=config.VISUALIZATION_CONFIG['figsize'])\n",
    "    plt.suptitle(f\"Training Curves - {best_model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Training history not found at {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1b4",
   "metadata": {},
   "source": [
    "## 10. Per-Class Performance Analysis\n",
    "\n",
    "Analyze the performance of the best model for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-class metrics from results_df\n",
    "if class_names is not None:\n",
    "    per_class_metrics = {}\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_columns = [col for col in results_df.columns if col.startswith(f\"{class_name}_\")]\n",
    "        if class_columns:\n",
    "            per_class_metrics[class_name] = results_df.loc[results_df['model_name'] == best_model_name, class_columns].iloc[0].to_dict()\n",
    "    \n",
    "    if per_class_metrics:\n",
    "        # Create DataFrame for visualization\n",
    "        per_class_df = pd.DataFrame()\n",
    "        for class_name, metrics in per_class_metrics.items():\n",
    "            metrics_dict = {'class': class_name}\n",
    "            for metric_name, value in metrics.items():\n",
    "                metric_key = metric_name.split('_')[-1]  # Extract metric name (precision, recall, etc.)\n",
    "                metrics_dict[metric_key] = value\n",
    "            per_class_df = pd.concat([per_class_df, pd.DataFrame([metrics_dict])], ignore_index=True)\n",
    "        \n",
    "        # Display per-class metrics\n",
    "        print(f\"Per-class performance for {best_model_name}:\")\n",
    "        display(per_class_df)\n",
    "        \n",
    "        # Plot per-class F1 scores\n",
    "        plt.figure(figsize=config.VISUALIZATION_CONFIG['figsize'])\n",
    "        sns.barplot(x='f1-score', y='class', data=per_class_df.sort_values('f1-score', ascending=False))\n",
    "        plt.title(f'Per-Class F1 Scores - {best_model_name}')\n",
    "        plt.xlabel('F1 Score')\n",
    "        plt.ylabel('Class')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Per-class metrics not found in results.\")\n",
    "else:\n",
    "    print(\"Class names not available for per-class analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1b6",
   "metadata": {},
   "source": [
    "## 11. CNN vs. Sequence Model Impact Analysis\n",
    "\n",
    "Analyze the impact of different CNN and sequence model choices on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by CNN model and calculate mean performance\n",
    "cnn_impact = results_df.groupby('cnn_model')[['accuracy', 'f1']].mean().reset_index()\n",
    "cnn_impact = cnn_impact.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Group by sequence model and calculate mean performance\n",
    "seq_impact = results_df.groupby('seq_model')[['accuracy', 'f1']].mean().reset_index()\n",
    "seq_impact = seq_impact.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Plot CNN model impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='accuracy', y='cnn_model', data=cnn_impact)\n",
    "plt.title('CNN Model Impact on Accuracy (Averaged Across Sequence Models)')\n",
    "plt.xlabel('Mean Accuracy')\n",
    "plt.ylabel('CNN Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot sequence model impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='accuracy', y='seq_model', data=seq_impact)\n",
    "plt.title('Sequence Model Impact on Accuracy (Averaged Across CNN Models)')\n",
    "plt.xlabel('Mean Accuracy')\n",
    "plt.ylabel('Sequence Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1b8",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "Summarize the findings of the ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_accuracy': best_model_row['accuracy'],\n",
    "    'best_f1': best_model_row['f1'],\n",
    "    'mean_accuracy': results_df['accuracy'].mean(),\n",
    "    'std_accuracy': results_df['accuracy'].std(),\n",
    "    'best_cnn': cnn_impact.iloc[0]['cnn_model'],\n",
    "    'best_cnn_mean_accuracy': cnn_impact.iloc[0]['accuracy'],\n",
    "    'best_seq': seq_impact.iloc[0]['seq_model'],\n",
    "    'best_seq_mean_accuracy': seq_impact.iloc[0]['accuracy'],\n",
    "    'num_models_tested': len(results_df),\n",
    "    'accuracy_range': results_df['accuracy'].max() - results_df['accuracy'].min()\n",
    "}\n",
    "\n",
    "print(\"Ablation Study Summary:\")\n",
    "print(f\"Number of models tested: {summary['num_models_tested']}\")\n",
    "print(f\"Best model: {summary['best_model']}\")\n",
    "print(f\"Best accuracy: {summary['best_accuracy']:.4f}\")\n",
    "print(f\"Best F1 score: {summary['best_f1']:.4f}\")\n",
    "print(f\"Mean accuracy across all models: {summary['mean_accuracy']:.4f} ± {summary['std_accuracy']:.4f}\")\n",
    "print(f\"Accuracy range: {summary['accuracy_range']:.4f}\")\n",
    "print(f\"Best CNN architecture: {summary['best_cnn']} (mean accuracy: {summary['best_cnn_mean_accuracy']:.4f})\")\n",
    "print(f\"Best sequence model: {summary['best_seq']} (mean accuracy: {summary['best_seq_mean_accuracy']:.4f})\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = os.path.join(results_dir, 'ablation_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump({k: float(v) if isinstance(v, (float, np.float32, np.float64)) else v for k, v in summary.items()}, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3e1ba",
   "metadata": {},
   "source": [
    "## 13. Save Results\n",
    "\n",
    "Save all results and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3e1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save results DataFrame to CSV\n",
    "results_csv_path = os.path.join(results_dir, 'ablation_results_detailed.csv')\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Detailed results saved to {results_csv_path}\")\n",
    "\n",
    "# Create a simplified results table for reporting\n",
    "simple_results = results_df[['model_name', 'cnn_model', 'seq_model', 'accuracy', 'precision', 'recall', 'f1']]\n",
    "simple_results = simple_results.sort_values('accuracy', ascending=False)\n",
    "simple_csv_path = os.path.join(results_dir, 'ablation_results_simple.csv')\n",
    "simple_results.to_csv(simple_csv_path, index=False)\n",
    "print(f\"Simplified results saved to {simple_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180fd27-38fd-474d-b1ac-b912d834d8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
