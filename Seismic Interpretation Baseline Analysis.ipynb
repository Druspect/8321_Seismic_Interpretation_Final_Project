{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22969c04aba9591e",
   "metadata": {},
   "source": [
    "# Seismic Interpretation Baseline Analysis\n",
    "## Jupyter Notebook: Baseline Models\n",
    "\n",
    "This Jupyter Notebook sets up and runs initial baseline models for seismic facies classification. Using a simple\n",
    "Convolutional Neural Network and an adapted pre-trained RoBERTa(TBD might use more advanced) model as benchmarks. This\n",
    "includes the demonstrate training loops, evaluation metrics, and plotting of training history.\n",
    "\n",
    "However this docuemnt is going to exclude the combination of the cnn with the pretrained mdoel, the optimization of the methodology to get the best seismic stratigraphy model, and reinforcement learning that will be used to train for different geologic regions and strata."
   ]
  },
  {
   "cell_type": "code",
   "id": "f88e19c83d6cb00a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:28:49.693525Z",
     "start_time": "2025-04-22T14:27:13.662481Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "!pip install segyio scipy\n",
    "import segyio\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "segy_path = \"Seismic_data.sgy\"\n",
    "tops_path = \"f3_dataset.sgy\"\n",
    "import torch.nn as nn\n",
    "\n",
    "# CNN history\n",
    "#cnn_train_loss, cnn_val_loss = [], []\n",
    "#cnn_train_acc,  cnn_val_acc  = [], []\n",
    "\n",
    "# RoBERTa history\n",
    "#roberta_train_loss, roberta_val_loss = [], []\n",
    "#roberta_train_acc,  roberta_val_acc  = [], []\n",
    "\n",
    "# 1) Load raw seismic cube to be analyzed\n",
    "with segyio.open(segy_path, \"r\", ignore_geometry=True) as f:\n",
    "    f.mmap()\n",
    "    try:\n",
    "        volume = segyio.tools.cube(f)  # shape (n_ilines, n_xlines, n_samples)\n",
    "    except Exception:\n",
    "        # fallback: build cube from traces + headers\n",
    "        n_traces  = f.tracecount\n",
    "        n_samples = len(f.samples)\n",
    "        raw       = np.stack([f.trace.raw[i] for i in range(n_traces)], axis=0)\n",
    "        inlines   = f.attributes(segyio.TraceField.INLINE_3D)[:]\n",
    "        xlines    = f.attributes(segyio.TraceField.CROSSLINE_3D)[:]\n",
    "        uni_il    = np.unique(inlines)\n",
    "        uni_xl    = np.unique(xlines)\n",
    "        n_il, n_xl= len(uni_il), len(uni_xl)\n",
    "        volume    = np.zeros((n_il, n_xl, n_samples), dtype=raw.dtype)\n",
    "        il_map    = {val: idx for idx, val in enumerate(uni_il)}\n",
    "        xl_map    = {val: idx for idx, val in enumerate(uni_xl)}\n",
    "        for t in range(n_traces):\n",
    "            iidx = il_map[inlines[t]]\n",
    "            xidx = xl_map[xlines[t]]\n",
    "            volume[iidx, xidx, :] = raw[t]\n",
    "\n",
    "# 2) Band‑pass + envelope extraction\n",
    "def bandpass_filter(trace, lowcut=5, highcut=60, fs=250, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    b, a = butter(order, [lowcut/nyq, highcut/nyq], btype=\"band\")\n",
    "    return filtfilt(b, a, trace)\n",
    "\n",
    "n_ilines, n_xlines, n_samples = volume.shape\n",
    "vol_bp = np.zeros_like(volume, dtype=np.float32)\n",
    "\n",
    "for il in range(n_ilines):\n",
    "    for xl in range(n_xlines):\n",
    "        tr    = volume[il, xl, :].astype(np.float32)\n",
    "        filt  = bandpass_filter(tr, lowcut=5, highcut=60, fs=250)\n",
    "        env   = np.abs(hilbert(filt))\n",
    "        vol_bp[il, xl, :] = env\n",
    "\n",
    "# 3) Outlier clipping + normalization\n",
    "p1, p99  = np.percentile(vol_bp, [1, 99])\n",
    "vol_bp   = np.clip(vol_bp, p1, p99)\n",
    "mean, std= vol_bp.mean(), vol_bp.std()\n",
    "vol_norm = (vol_bp - mean) / (std + 1e-8)\n",
    "\n",
    "print(f\"Preprocessed volume shape={vol_norm.shape}, mean={vol_norm.mean():.3f}, std={vol_norm.std():.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segyio in /opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages (1.9.13)\r\n",
      "Requirement already satisfied: scipy in /opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages (1.13.1)\r\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages (from segyio) (1.23.5)\r\n",
      "Preprocessed volume shape=(651, 951, 462), mean=0.000, std=1.000\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This cell implements a complete preprocessing pipeline that converts a raw SEG‑Y volume into a normalized feature cube suitable for patch extraction and CNN training. First, the necessary libraries are imported and installed—NumPy and PyTorch for array and tensor operations, segyio for efficient memory‑mapped reading of seismic files, and SciPy’s signal module for band‑pass filtering and envelope extraction. Random seeds in NumPy and PyTorch are fixed to ensure reproducibility of any stochastic routines. The primary SEG‑Y file is then opened in read mode with segyio and memory‑mapped; `segyio.tools.cube` uses the inline/crossline headers to assemble a three‑dimensional array of shape `(n_inlines, n_crosslines, n_samples)`. If header information is unavailable or malformed, each raw trace is read, its inline and crossline coordinates extracted, and the full volume reconstructed manually via dictionary lookups.\n",
    "\n",
    "Once the raw 3D volume is obtained, every inline×crossline trace undergoes a fourth‑order Butterworth band‑pass\n",
    "filter between 5Hz and 60Hz (zero‑phase forward/backward filtering via `filtfilt` to prevent phase distortion). The\n",
    "analytic signal of the filtered trace is computed using the Hilbert transform, and its absolute value (the envelope)\n",
    "is extracted to emphasize reflector strength, a key stratigraphic attribute. These envelopes populate the temporary\n",
    "cube `vol_bp`. To mitigate the influence of acquisition spikes and extreme outliers, the 1st and 99th percentiles of\n",
    "all envelope values are computed and any values outside this range are clipped. Finally, a global standardization is\n",
    "applied by subtracting the mean and dividing by the standard deviation (with a small epsilon added to avoid division\n",
    "by zero), producing `vol_norm`, which exhibit zero mean and unit variance. A print statement then confirms the\n",
    "preprocessed volume’s dimensions and verifies that its mean and standard deviation are approximately 0 and 1,\n",
    "respectively—indicating readiness for the downstream analysis."
   ],
   "id": "d8cf2c84b59990f6"
  },
  {
   "cell_type": "markdown",
   "id": "123880a29131d52a",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "a766f569c09b3e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:28:52.673404Z",
     "start_time": "2025-04-22T14:28:49.806795Z"
    }
   },
   "source": [
    "# Code from segyio paper to load tops\n",
    "with segyio.open(tops_path, \"r\", ignore_geometry=True) as f2:\n",
    "    f2.mmap()\n",
    "    try:\n",
    "        tops_vol = segyio.tools.cube(f2)  # shape might be (n_il, n_xl, 1) or (n_il, n_xl, n_samples)\n",
    "    except Exception:\n",
    "        # Fallback: stack raw traces and reshape based on inline/crossline headers\n",
    "        n_traces = f2.tracecount\n",
    "        n_samples = len(f2.samples)\n",
    "        raw = np.stack([f2.trace.raw[i] for i in range(n_traces)], axis=0)\n",
    "        inlines = f2.attributes(segyio.TraceField.INLINE_3D)[:]\n",
    "        xlines = f2.attributes(segyio.TraceField.CROSSLINE_3D)[:]\n",
    "        uni_il = np.unique(inlines)\n",
    "        uni_xl = np.unique(xlines)\n",
    "        il_map = {v: i for i, v in enumerate(uni_il)}\n",
    "        xl_map = {v: i for i, v in enumerate(uni_xl)}\n",
    "        tops_vol = np.zeros((len(uni_il), len(uni_xl), n_samples), dtype=raw.dtype)\n",
    "        for t in range(n_traces):\n",
    "            i = il_map[inlines[t]]\n",
    "            j = xl_map[xlines[t]]\n",
    "            tops_vol[i, j, :] = raw[t]\n",
    "\n",
    "# Now reduce to a 2D pick index array:\n",
    "if tops_vol.ndim == 3 and tops_vol.shape[2] == 1:\n",
    "    tops = tops_vol[:, :, 0]\n",
    "elif tops_vol.ndim == 3:\n",
    "    # e.g. picks stored as amplitude across samples—take the mean and round\n",
    "    tops = np.round(tops_vol.mean(axis=2)).astype(int)\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected tops_vol shape: {tops_vol.shape}\")\n",
    "\n",
    "print(\"Loaded tops shape:\", tops.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tops shape: (651, 951)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "850114b7546b05c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:28:56.235208Z",
     "start_time": "2025-04-22T14:28:52.718964Z"
    }
   },
   "source": [
    "# parameters\n",
    "patch_size = 32\n",
    "stride     = 32\n",
    "image_size  = patch_size   # for the CNN flatten\n",
    "num_classes = 2            # binary facies to be changed later for multi-class context\n",
    "n_il, n_xl, n_sm = volume.shape\n",
    "\n",
    "# normalize entire volume to zero-mean/unit-variance\n",
    "vol_mean = volume.mean()\n",
    "vol_std  = volume.std()\n",
    "vol_norm = (volume - vol_mean) / (vol_std + 1e-8)\n",
    "\n",
    "patches = []\n",
    "labels  = []\n",
    "\n",
    "for il in range(n_il):\n",
    "    for xl in range(0, n_xl - patch_size + 1, stride):\n",
    "        for sm in range(0, n_sm - patch_size + 1, stride):\n",
    "            patch = vol_norm[il, xl:xl+patch_size, sm:sm+patch_size]\n",
    "            patches.append(patch)\n",
    "            # label = 1 if patch center depth (sample index) is below the horizon pick\n",
    "            top_idx      = tops[il, xl]\n",
    "            center_depth = sm + patch_size//2\n",
    "            labels.append(int(center_depth > top_idx))\n",
    "\n",
    "# stack & reshape for PyTorch (N, C=1, H, W)\n",
    "X = np.stack(patches)[:, None, :, :].astype(np.float32)\n",
    "y = np.array(labels, dtype=np.int64)\n",
    "\n",
    "# check if the data is balanced and set num_classes which corresponds to the number of stratigraphic classes(bed types)\n",
    "unique_classes = np.unique(y)\n",
    "num_classes    = len(unique_classes)\n",
    "print(f\"Detected classes: {unique_classes}  →  num_classes = {num_classes}\")\n",
    "\n",
    "# train/val split (80/20) and DataLoader\n",
    "split = int(0.8 * len(y))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "val_ds   = TensorDataset(torch.tensor(X_val),   torch.tensor(y_val))\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Prepared\", len(train_ds), \"train and\", len(val_ds), \"val patches.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: [0 1]  →  num_classes = 2\n",
      "Prepared 211444 train and 52862 val patches.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "ccb2e8276655b40e",
   "metadata": {},
   "source": [
    "Simple CNN Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "9cf5e6775a9daf95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:28:56.255170Z",
     "start_time": "2025-04-22T14:28:56.247025Z"
    }
   },
   "source": [
    "# Define a simple CNN (based off previous work with cnns)\n",
    "class SeismicCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=num_classes, patch_size=32):\n",
    "        super().__init__()\n",
    "        # Feature extractor: two conv→BN→ReLU→pool blocks\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),  # 32×32→32×32\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                                       # →16×16\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),           # 16×16→16×16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),                                       # →8×8\n",
    "        )\n",
    "        # Compute flattened feature size after two pools\n",
    "        feat_dim = patch_size // 4  # 32→16→8\n",
    "        flat_size = 32 * feat_dim * feat_dim\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flat_size, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# init\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cnn_model = SeismicCNN().to(device)\n",
    "criterion   = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = torch.optim.Adam(cnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "print(cnn_model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeismicCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=64, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "f9a14990ed9684e8",
   "metadata": {},
   "source": [
    "Pre-trained RoBERTa Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "91210f28c0b7b5c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T14:31:58.800871Z",
     "start_time": "2025-04-22T14:28:56.289743Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "# Prepare text representations of images for RoBERTa\n",
    "X_text_train = []\n",
    "for img in X_train:\n",
    "    # Flatten image and convert pixel values to tokens (\"bright\"/\"dark\")\n",
    "    pixels = img.flatten()\n",
    "    tokens = [\"bright\" if px > 0.5 else \"dark\" for px in pixels]\n",
    "    X_text_train.append(\" \".join(tokens))\n",
    "X_text_val = []\n",
    "for img in X_val:\n",
    "    pixels = img.flatten()\n",
    "    tokens = [\"bright\" if px > 0.5 else \"dark\" for px in pixels]\n",
    "    X_text_val.append(\" \".join(tokens))\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "roberta_model = roberta_model.to(device)\n",
    "optimizer_roberta = torch.optim.Adam(roberta_model.parameters(), lr=1e-5)\n",
    "\n",
    "print(roberta_model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "af4df8705a4cb644",
   "metadata": {},
   "source": [
    "Training Loop for CNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e2d81ef3dc992df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T16:34:19.327159Z",
     "start_time": "2025-04-22T15:00:55.912855Z"
    }
   },
   "source": [
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1) Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32]\n",
    "}\n",
    "\n",
    "# 2) Prepare data tensors (flatten patches into X_all, y_all)\n",
    "X_all = torch.tensor(X, dtype=torch.float32)  # X from preprocessing: shape (N,1,H,W)\n",
    "y_all = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# 3) Set up 5‑fold CV\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "# 4) Grid search with CV\n",
    "for params in ParameterGrid(param_grid):\n",
    "    fold_scores = []\n",
    "    for train_idx, val_idx in kf.split(X_all):\n",
    "        # Create loaders for this fold\n",
    "        train_ds = TensorDataset(X_all[train_idx], y_all[train_idx])\n",
    "        val_ds   = TensorDataset(X_all[val_idx],   y_all[val_idx])\n",
    "        train_loader = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=params[\"batch_size\"])\n",
    "\n",
    "        # Instantiate a fresh model each fold\n",
    "        model = SeismicCNN().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train for a small number of epochs (e.g. 5) per fold\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            for Xb, yb in train_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(Xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate on validation fold\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                logits = model(Xb)\n",
    "                preds  = logits.argmax(dim=1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total   += yb.size(0)\n",
    "        fold_scores.append(correct / total)\n",
    "\n",
    "    # Compute average CV score\n",
    "    avg_score = sum(fold_scores) / len(fold_scores)\n",
    "    print(f\"Params {params} → CV accuracy: {avg_score:.3f}\")\n",
    "    if avg_score > best_score:\n",
    "        best_score  = avg_score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {best_params} with CV accuracy {best_score:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params {'batch_size': 16, 'lr': 0.001} → CV accuracy: 0.989\n",
      "Params {'batch_size': 16, 'lr': 0.0005} → CV accuracy: 0.989\n",
      "Params {'batch_size': 32, 'lr': 0.001} → CV accuracy: 0.989\n",
      "Params {'batch_size': 32, 'lr': 0.0005} → CV accuracy: 0.989\n",
      "\n",
      "Best hyperparameters: {'batch_size': 16, 'lr': 0.0005} with CV accuracy 0.989\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "44a6f2f124e31f21",
   "metadata": {},
   "source": [
    "Training Loop for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0ab7ec5ce94bbdf",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-22T16:34:20.550226Z"
    }
   },
   "source": [
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AdamW, get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "TOKENIZERS_PARALLELISM=(True | False)\n",
    "\n",
    "# 1) Load and configure model/tokenizer\n",
    "config    = AutoConfig.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "# 2) Pre‑tokenize once\n",
    "MAX_LEN = 256\n",
    "train_enc = tokenizer(X_text_train, padding=\"max_length\", truncation=True,\n",
    "                      max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "val_enc   = tokenizer(X_text_val,   padding=\"max_length\", truncation=True,\n",
    "                      max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "train_ds = TensorDataset(train_enc.input_ids, train_enc.attention_mask, torch.tensor(y_train))\n",
    "val_ds   = TensorDataset(val_enc.input_ids,   val_enc.attention_mask,   torch.tensor(y_val))\n",
    "\n",
    "# 3) DataLoaders with minor speed tweaks\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
    "                          pin_memory=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16,\n",
    "                          pin_memory=True, num_workers=2)\n",
    "\n",
    "# 4) Device, optimizer, scheduler, scaler\n",
    "device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"mps\") if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "total_steps   = len(train_loader) * 3  # 3 epochs\n",
    "warmup_steps  = int(0.1 * total_steps)\n",
    "scheduler     = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "scaler = GradScaler()  # for mixed precision\n",
    "\n",
    "# 5) Training loop with mixed precision, clipping, scheduler\n",
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # — Training —\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    for input_ids, attn_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} Train\"):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        labels    = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=(device.type==\"cuda\")):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
    "            loss    = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss    += loss.item() * labels.size(0)\n",
    "        train_correct += (outputs.logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_ds)\n",
    "    train_acc      = train_correct / len(train_ds)\n",
    "\n",
    "    # — Validation —\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn_mask, labels in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} Val\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            labels    = labels.to(device)\n",
    "            logits    = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "            val_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    val_acc = val_correct / len(val_ds)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch}/{EPOCHS} ▶ \"\n",
    "        f\"Train loss: {avg_train_loss:.4f}, Train acc: {train_acc:.3f}; \"\n",
    "        f\"Val acc: {val_acc:.3f}\\n\", flush=True\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/tensorflow_env/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/3 Train:   0%|          | 0/13216 [00:01<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f94445ce40e4ebcab997844d779ab1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1/3 Val:   0%|          | 0/3304 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e27a149ef60741879a039e4d02dfa0f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3 ▶ Train loss: 0.0679, Train acc: 0.985; Val acc: 0.991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2/3 Train:   0%|          | 0/13216 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d375625bf5b14888ac0d639cc45be47f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4a29d055e02a6ae",
   "metadata": {},
   "source": [
    "Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "76859e7947463bd2",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# 1) Define distinct validation loaders\n",
    "cnn_val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# Assuming you pre‑tokenized X_text_val into val_enc earlier:\n",
    "roberta_val_loader = DataLoader(\n",
    "    TensorDataset(\n",
    "        val_enc.input_ids,\n",
    "        val_enc.attention_mask,\n",
    "        torch.tensor(y_val, dtype=torch.long)\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 2) Evaluate CNN\n",
    "cnn_model.eval()\n",
    "y_true = y_val  # numpy array\n",
    "y_pred_cnn, y_score_cnn = [], []\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in cnn_val_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        logits = cnn_model(Xb)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        preds  = probs.argmax(dim=1)\n",
    "        y_pred_cnn.extend(preds.cpu().numpy())\n",
    "        y_score_cnn.extend(probs[:, 1].cpu().numpy())  # class‑1 probability\n",
    "\n",
    "# 3) Evaluate RoBERTa\n",
    "roberta_model.eval()\n",
    "y_pred_rob, y_score_rob = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attn_mask, labels in roberta_val_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        logits    = roberta_model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "        probs     = torch.softmax(logits, dim=1)\n",
    "        preds     = probs.argmax(dim=1)\n",
    "        y_pred_rob.extend(preds.cpu().numpy())\n",
    "        y_score_rob.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "# 4) Compute metrics\n",
    "acc_cnn    = accuracy_score(y_true,      y_pred_cnn)\n",
    "prec_cnn   = precision_score(y_true,     y_pred_cnn, average='binary')\n",
    "recall_cnn = recall_score(y_true,        y_pred_cnn, average='binary')\n",
    "f1_cnn     = f1_score(y_true,            y_pred_cnn, average='binary')\n",
    "auc_cnn    = roc_auc_score(y_true,       y_score_cnn)\n",
    "\n",
    "acc_rob    = accuracy_score(y_true,      y_pred_rob)\n",
    "prec_rob   = precision_score(y_true,     y_pred_rob, average='binary')\n",
    "recall_rob = recall_score(y_true,        y_pred_rob, average='binary')\n",
    "f1_rob     = f1_score(y_true,            y_pred_rob, average='binary')\n",
    "auc_rob    = roc_auc_score(y_true,       y_score_rob)\n",
    "\n",
    "print(f\"CNN     ▶ Acc: {acc_cnn:.3f}, Precision: {prec_cnn:.3f}, Recall: {recall_cnn:.3f}, F1: {f1_cnn:.3f}, AUC: {auc_cnn:.3f}\")\n",
    "print(f\"RoBERTa ▶ Acc: {acc_rob:.3f}, Precision: {prec_rob:.3f}, Recall: {recall_rob:.3f}, F1: {f1_rob:.3f}, AUC: {auc_rob:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21191bde9be2336f",
   "metadata": {},
   "source": "Plotting CNN and Roberta Training"
  },
  {
   "cell_type": "code",
   "id": "91637ae35aaef912",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Safety checks ---\n",
    "missing = [name for name in (\"cnn_train_loss\", \"cnn_val_loss\", \"cnn_train_acc\", \"cnn_val_acc\") if name not in globals()]\n",
    "if missing:\n",
    "    raise NameError(f\"Missing CNN history variables: {missing}. Run the CNN training loop first.\")\n",
    "\n",
    "# roberta history is optional\n",
    "has_rob = all(name in globals() for name in (\"roberta_train_loss\", \"roberta_val_loss\", \"roberta_train_acc\", \"roberta_val_acc\"))\n",
    "\n",
    "# --- Epoch counts ---\n",
    "num_epochs_cnn = len(cnn_train_loss)\n",
    "num_epochs_rob = len(roberta_train_loss) if has_rob else 0\n",
    "\n",
    "# --- Set up subplots ---\n",
    "# If RoB history exists: 2 rows (CNN, RoB), 2 cols (loss, acc).\n",
    "# Otherwise: only CNN row.\n",
    "nrows = 2 if has_rob else 1\n",
    "fig, axes = plt.subplots(nrows, 2, figsize=(12, 4*nrows))\n",
    "\n",
    "# Ensure axes is 2D array\n",
    "if nrows == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "\n",
    "# --- CNN: Loss ---\n",
    "axes[0, 0].plot(range(1, num_epochs_cnn+1), cnn_train_loss, label=\"Train Loss\")\n",
    "axes[0, 0].plot(range(1, num_epochs_cnn+1), cnn_val_loss,   label=\"Val Loss\")\n",
    "axes[0, 0].set_title(\"CNN Loss\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# --- CNN: Accuracy ---\n",
    "axes[0, 1].plot(range(1, num_epochs_cnn+1), [a*100 for a in cnn_train_acc], label=\"Train Acc\")\n",
    "axes[0, 1].plot(range(1, num_epochs_cnn+1), [a*100 for a in cnn_val_acc],   label=\"Val Acc\")\n",
    "axes[0, 1].set_title(\"CNN Accuracy (%)\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "if has_rob:\n",
    "    # --- RoBerta: Loss ---\n",
    "    axes[1, 0].plot(range(1, num_epochs_rob+1), roberta_train_loss, label=\"Train Loss\")\n",
    "    axes[1, 0].plot(range(1, num_epochs_rob+1), roberta_val_loss,   label=\"Val Loss\")\n",
    "    axes[1, 0].set_title(\"RoBerta Loss\")\n",
    "    axes[1, 0].set_xlabel(\"Epoch\")\n",
    "    axes[1, 0].set_ylabel(\"Loss\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # --- RoBerta: Accuracy ---\n",
    "    axes[1, 1].plot(range(1, num_epochs_rob+1), [a*100 for a in roberta_train_acc], label=\"Train Acc\")\n",
    "    axes[1, 1].plot(range(1, num_epochs_rob+1), [a*100 for a in roberta_val_acc],   label=\"Val Acc\")\n",
    "    axes[1, 1].set_title(\"RoBerta Accuracy (%)\")\n",
    "    axes[1, 1].set_xlabel(\"Epoch\")\n",
    "    axes[1, 1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
